\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{eq: grad fj}{{1}{1}{Q1: Analysis of the objective function}{equation.0.1}{}}
\newlabel{eq: linear}{{3}{2}{Q1: Analysis of the objective function}{equation.0.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Decomposition of the parameter space. In blue, the line with $ x_j $ means the line $ b = x_j a $. In red, the number indicates which points are activated, meaning that $ ax_j - b > 0 $. For example, '2345' means $ x_2,x_3,x_4,x_5 $ are activated. Finally, stationary points are in green. Note that they consist of a flat region, a line (with $ b=ax_5-y_5 $ where $ a>\pi $) and the global minimizer $ x^{*} $.\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: space decomposition}{{1}{2}{Decomposition of the parameter space. In blue, the line with $ x_j $ means the line $ b = x_j a $. In red, the number indicates which points are activated, meaning that $ ax_j - b > 0 $. For example, '2345' means $ x_2,x_3,x_4,x_5 $ are activated. Finally, stationary points are in green. Note that they consist of a flat region, a line (with $ b=ax_5-y_5 $ where $ a>\pi $) and the global minimizer $ x^{*} $.\relax }{figure.caption.2}{}}
\newlabel{fig: vecfield_global}{{2a}{3}{Global view.\relax }{figure.caption.5}{}}
\newlabel{sub@fig: vecfield_global}{{a}{3}{Global view.\relax }{figure.caption.5}{}}
\newlabel{fig: vecfield_veryclose}{{2b}{3}{Local view\relax }{figure.caption.5}{}}
\newlabel{sub@fig: vecfield_veryclose}{{b}{3}{Local view\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The vector field around the minimizer and the initial point. At each point, the slope corresponds to the minus gradient of $ f $.\relax }}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig: vector field}{{2}{3}{The vector field around the minimizer and the initial point. At each point, the slope corresponds to the minus gradient of $ f $.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Minimal stepsize $ \alpha ^{*}$ that the iterates end up in the flat region}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$ \alpha < \alpha ^{*}$ does not necessarily results in convergence}{3}{section*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Trajectories when $ \alpha = 0.99\alpha ^{*} $. The first step misses the boundary of the flat region and the iterates bounce between the two sides of the global minimizer without ever convergence to it. \relax }}{4}{figure.caption.7}\protected@file@percent }
\newlabel{fig: Q2_99}{{3}{4}{Trajectories when $ \alpha = 0.99\alpha ^{*} $. The first step misses the boundary of the flat region and the iterates bounce between the two sides of the global minimizer without ever convergence to it. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Finding the largest stepsize $ \alpha $ for convergence}{4}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{But what is the best stepsize?}{4}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Various convergence modes. Except for the first case ($ \alpha =1.32 $) which doesn't converge, several convergence modes (with or without oscillating) are shown.\relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig: Q2_various}{{4}{5}{Various convergence modes. Except for the first case ($ \alpha =1.32 $) which doesn't converge, several convergence modes (with or without oscillating) are shown.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Remark}{6}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The number of iterations needed to achieve $ ||\nabla f||<1e-6 $ for various $ \alpha $. Observe how the number of iterations first drop gently and then increase abruptly. $ \alpha = 1.2652 $ needs 181 iterations, which is fastest. \relax }}{6}{figure.caption.12}\protected@file@percent }
\newlabel{fig:Q2_iteration_alpha}{{5}{6}{The number of iterations needed to achieve $ ||\nabla f||<1e-6 $ for various $ \alpha $. Observe how the number of iterations first drop gently and then increase abruptly. $ \alpha = 1.2652 $ needs 181 iterations, which is fastest. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The trajectories for the critical $ \alpha $ that results in an abrupt increase in the number of iterations needed for convergence. Observe that there is a \textbf  {phase transition} in the geometry of the trajectories. Although oscillating behavior appears in both cases, notice that when $ \alpha = 1.2652 $, two oscillating curves converge in the same direction so there is not a lot waste in oscillating (and since $ \alpha = 1.2652 $ is large, convergence is fast). However, when $ \alpha = 1.2988 $, the two oscillating curves are converging in the opposite directions (so they are always oscillating to each other, wasteful!), which explains the sudden drop in the convergence speed.\relax }}{7}{figure.caption.13}\protected@file@percent }
\newlabel{fig: Q2_increase}{{6}{7}{The trajectories for the critical $ \alpha $ that results in an abrupt increase in the number of iterations needed for convergence. Observe that there is a \textbf {phase transition} in the geometry of the trajectories. Although oscillating behavior appears in both cases, notice that when $ \alpha = 1.2652 $, two oscillating curves converge in the same direction so there is not a lot waste in oscillating (and since $ \alpha = 1.2652 $ is large, convergence is fast). However, when $ \alpha = 1.2988 $, the two oscillating curves are converging in the opposite directions (so they are always oscillating to each other, wasteful!), which explains the sudden drop in the convergence speed.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Which strategy should be used?}{7}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GD works better than SGD in the problem}{7}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Absolute error (with respect to the global minimizer $ x^{\ast } $) versus iterations, when $ M = 1000 $. "Double descent" occurs when $ \alpha \leq 2 $. The plot is averaged over 1000 trajectories.\relax }}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig: Q3_iteration}{{7}{8}{Absolute error (with respect to the global minimizer $ x^{\ast } $) versus iterations, when $ M = 1000 $. "Double descent" occurs when $ \alpha \leq 2 $. The plot is averaged over 1000 trajectories.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Histogram of absolute error (with respect to the global minimizer $ x^{\ast } $), when $ M = 1000 $. The plot is averaged over 1000 trajectories. When $ \alpha _0 = 4.65 $, there is relatively high probability that SGD will not converge to $ x^{*} $.\relax }}{9}{figure.caption.16}\protected@file@percent }
\newlabel{fig: Q3_histogram}{{8}{9}{Histogram of absolute error (with respect to the global minimizer $ x^{\ast } $), when $ M = 1000 $. The plot is averaged over 1000 trajectories. When $ \alpha _0 = 4.65 $, there is relatively high probability that SGD will not converge to $ x^{*} $.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces One trajectory for SGD. When $ \alpha _0 = 0.3 $, the iterates are stuck in the region where the gradient is small and therefore progress extremely slowly. Using larger stepsize such as $ \alpha _0 =2$ speeds up convergence significantly.\relax }}{10}{figure.caption.19}\protected@file@percent }
\newlabel{fig: Q3_choose_alpha}{{9}{10}{One trajectory for SGD. When $ \alpha _0 = 0.3 $, the iterates are stuck in the region where the gradient is small and therefore progress extremely slowly. Using larger stepsize such as $ \alpha _0 =2$ speeds up convergence significantly.\relax }{figure.caption.19}{}}
